{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Sign Language Translator - Google Colab Training Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NKEthio/Sign_Language_Translator/blob/main/Sign_Language_Translator_Colab.ipynb)\n",
    "\n",
    "This notebook provides a complete end-to-end workflow for training a Sign Language (ASL) alphabet classifier using PyTorch.\n",
    "\n",
    "## What you'll learn:\n",
    "- Setting up the environment in Google Colab\n",
    "- Downloading and preparing the Sign Language MNIST dataset from Kaggle\n",
    "- Training a classifier with pretrained backbones (ResNet, MobileNet, EfficientNet)\n",
    "- Evaluating model performance\n",
    "- Running inference on test images\n",
    "- Saving and exporting trained models\n",
    "\n",
    "**GPU Recommendation**: For faster training, enable GPU in Runtime > Change runtime type > Hardware accelerator > GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's clone the repository and install all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/NKEthio/Sign_Language_Translator.git\n",
    "%cd Sign_Language_Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_package"
   },
   "outputs": [],
   "source": [
    "# Install the package and dependencies\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_install"
   },
   "outputs": [],
   "source": [
    "# Verify installation and check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Import the package to verify it works\n",
    "from sign_language_translator import (\n",
    "    build_classifier,\n",
    "    ModelConfig,\n",
    "    build_train_transforms,\n",
    "    build_eval_transforms,\n",
    "    train_one_epoch,\n",
    "    evaluate,\n",
    "    set_seed,\n",
    ")\n",
    "print(\"\\nâœ“ Package imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaggle_setup"
   },
   "source": [
    "## 2. Download Dataset from Kaggle\n",
    "\n",
    "We'll use the Sign Language MNIST dataset from Kaggle. To download it, you need to:\n",
    "\n",
    "1. Get your Kaggle API credentials:\n",
    "   - Go to https://www.kaggle.com/account\n",
    "   - Click \"Create New API Token\" to download `kaggle.json`\n",
    "   - Upload the file when prompted below\n",
    "\n",
    "2. The dataset contains 24 classes (letters A-Z excluding J and Z which require motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_kaggle_key"
   },
   "outputs": [],
   "source": [
    "# Upload kaggle.json file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Please upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "print(\"âœ“ Kaggle credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_dataset"
   },
   "outputs": [],
   "source": [
    "# Download the Sign Language MNIST dataset\n",
    "!kaggle datasets download -d datamunge/sign-language-mnist\n",
    "!unzip -q sign-language-mnist.zip -d sign_language_data\n",
    "!ls -lh sign_language_data/\n",
    "print(\"\\nâœ“ Dataset downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 3. Convert CSV Data to Images\n",
    "\n",
    "The dataset comes as CSV files. We'll convert them to images in ImageFolder format for easier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert_data"
   },
   "outputs": [],
   "source": [
    "# Convert CSV to ImageFolder format\n",
    "!slt-convert \\\n",
    "  --train_csv sign_language_data/sign_mnist_train.csv \\\n",
    "  --test_csv sign_language_data/sign_mnist_test.csv \\\n",
    "  --out_dir data/sign_mnist\n",
    "\n",
    "print(\"\\nâœ“ Data conversion complete!\")\n",
    "!ls -l data/sign_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect_data"
   },
   "outputs": [],
   "source": [
    "# Inspect the dataset structure\n",
    "import os\n",
    "\n",
    "train_dir = \"data/sign_mnist/train\"\n",
    "test_dir = \"data/sign_mnist/test\"\n",
    "\n",
    "classes = sorted(os.listdir(train_dir))\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Count images per class\n",
    "for cls in classes[:5]:  # Show first 5 classes\n",
    "    train_count = len(os.listdir(os.path.join(train_dir, cls)))\n",
    "    test_count = len(os.listdir(os.path.join(test_dir, cls))) if os.path.exists(os.path.join(test_dir, cls)) else 0\n",
    "    print(f\"Class '{cls}': {train_count} train, {test_count} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## 4. Visualize Sample Images\n",
    "\n",
    "Let's visualize some sample images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_samples"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Show sample images from different classes\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "fig.suptitle('Sample Sign Language Images', fontsize=16)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < len(classes):\n",
    "        cls = classes[idx]\n",
    "        cls_dir = os.path.join(train_dir, cls)\n",
    "        img_files = os.listdir(cls_dir)\n",
    "        img_path = os.path.join(cls_dir, random.choice(img_files))\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Class: {cls}')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Set up the training parameters. You can modify these based on your needs:\n",
    "\n",
    "- **backbone**: Model architecture (`resnet18`, `resnet34`, `mobilenet_v3_small`, `mobilenet_v3_large`, `efficientnet_b0`)\n",
    "- **epochs**: Number of training epochs\n",
    "- **batch_size**: Batch size for training\n",
    "- **learning_rate**: Learning rate for optimizer\n",
    "- **image_size**: Input image size (default: 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BACKBONE = \"resnet18\"  # Options: resnet18, resnet34, mobilenet_v3_small, mobilenet_v3_large, efficientnet_b0\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "IMAGE_SIZE = 224\n",
    "GRAYSCALE = True  # Sign MNIST is grayscale\n",
    "OUTPUT_DIR = \"artifacts\"\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Backbone: {BACKBONE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE}\")\n",
    "print(f\"  Grayscale: {GRAYSCALE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model"
   },
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Now let's train the model using the CLI tool or custom Python code.\n",
    "\n",
    "### Option A: Using CLI Tool (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_cli"
   },
   "outputs": [],
   "source": [
    "# Train using the CLI tool\n",
    "!slt-train \\\n",
    "  --data_dir data/sign_mnist/train \\\n",
    "  --val_dir data/sign_mnist/test \\\n",
    "  --backbone {BACKBONE} \\\n",
    "  --epochs {EPOCHS} \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --lr {LEARNING_RATE} \\\n",
    "  --image_size {IMAGE_SIZE} \\\n",
    "  --grayscale \\\n",
    "  --output_dir {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom"
   },
   "source": [
    "### Option B: Custom Training Loop (for more control)\n",
    "\n",
    "If you want more control over the training process, you can use the Python API directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_python"
   },
   "outputs": [],
   "source": [
    "# Custom training loop (alternative to CLI)\n",
    "# Uncomment to use this instead of the CLI tool\n",
    "\n",
    "# from pathlib import Path\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import nn\n",
    "# from sign_language_translator.datasets import ImageFolderConfig, build_imagefolder_dataset\n",
    "\n",
    "# # Set random seed\n",
    "# set_seed(42)\n",
    "\n",
    "# # Setup device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Create transforms\n",
    "# image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "# train_tfms = build_train_transforms(image_size=image_size, grayscale=GRAYSCALE)\n",
    "# val_tfms = build_eval_transforms(image_size=image_size, grayscale=GRAYSCALE)\n",
    "\n",
    "# # Create datasets\n",
    "# train_ds = build_imagefolder_dataset(\n",
    "#     ImageFolderConfig(root=Path(\"data/sign_mnist/train\"), image_size=image_size, grayscale=GRAYSCALE),\n",
    "#     transform=train_tfms,\n",
    "# )\n",
    "# val_ds = build_imagefolder_dataset(\n",
    "#     ImageFolderConfig(root=Path(\"data/sign_mnist/test\"), image_size=image_size, grayscale=GRAYSCALE),\n",
    "#     transform=val_tfms,\n",
    "# )\n",
    "\n",
    "# print(f\"Training samples: {len(train_ds)}\")\n",
    "# print(f\"Validation samples: {len(val_ds)}\")\n",
    "# print(f\"Number of classes: {len(train_ds.classes)}\")\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# # Build model\n",
    "# config = ModelConfig(\n",
    "#     backbone=BACKBONE,\n",
    "#     num_classes=len(train_ds.classes),\n",
    "#     pretrained=True,\n",
    "#     dropout=0.2,\n",
    "#     in_channels=1 if GRAYSCALE else 3,\n",
    "# )\n",
    "# model = build_classifier(config)\n",
    "# model.to(device)\n",
    "\n",
    "# print(f\"\\nModel: {BACKBONE}\")\n",
    "# print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# # Setup training\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# # Training loop\n",
    "# best_val_acc = 0.0\n",
    "# output_dir = Path(OUTPUT_DIR)\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(\"\\nStarting training...\")\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     print(f\"Epoch {epoch:03d}/{EPOCHS} | \"\n",
    "#           f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "#           f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "    \n",
    "#     # Save best model\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_path = output_dir / \"model_best.pt\"\n",
    "#         torch.save({\n",
    "#             \"epoch\": epoch,\n",
    "#             \"model_state\": model.state_dict(),\n",
    "#             \"classes\": train_ds.classes,\n",
    "#             \"backbone\": BACKBONE,\n",
    "#             \"grayscale\": GRAYSCALE,\n",
    "#             \"image_size\": image_size,\n",
    "#         }, best_path)\n",
    "#         print(f\"  âœ“ Saved new best model (val_acc={val_acc:.4f})\")\n",
    "\n",
    "# print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## 7. Evaluate the Trained Model\n",
    "\n",
    "Let's check the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_best_model"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(OUTPUT_DIR) / \"model_best.pt\"\n",
    "if model_path.exists():\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    print(\"Model checkpoint loaded:\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Backbone: {checkpoint['backbone']}\")\n",
    "    print(f\"  Classes: {len(checkpoint['classes'])}\")\n",
    "    print(f\"  Image size: {checkpoint['image_size']}\")\n",
    "    print(f\"  Grayscale: {checkpoint['grayscale']}\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix"
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix and classification report\n",
    "from sign_language_translator.datasets import ImageFolderConfig, build_imagefolder_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Reload model for evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Rebuild model\n",
    "config = ModelConfig(\n",
    "    backbone=checkpoint['backbone'],\n",
    "    num_classes=len(checkpoint['classes']),\n",
    "    pretrained=False,\n",
    "    dropout=0.2,\n",
    "    in_channels=1 if checkpoint['grayscale'] else 3,\n",
    ")\n",
    "model = build_classifier(config)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "val_tfms = build_eval_transforms(image_size=checkpoint['image_size'], grayscale=checkpoint['grayscale'])\n",
    "val_ds = build_imagefolder_dataset(\n",
    "    ImageFolderConfig(root=Path(\"data/sign_mnist/test\"), image_size=checkpoint['image_size'], grayscale=checkpoint['grayscale']),\n",
    "    transform=val_tfms,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Collect predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=checkpoint['classes']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=checkpoint['classes'], yticklabels=checkpoint['classes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 8. Test Inference on Sample Images\n",
    "\n",
    "Let's test the model on some sample images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_inference"
   },
   "outputs": [],
   "source": [
    "# Test inference on random samples\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Get random test samples\n",
    "num_samples = 8\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Model Predictions on Test Samples', fontsize=16)\n",
    "\n",
    "# Create transform\n",
    "transform = build_eval_transforms(image_size=checkpoint['image_size'], grayscale=checkpoint['grayscale'])\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < num_samples:\n",
    "        # Get random image\n",
    "        cls = random.choice(checkpoint['classes'])\n",
    "        cls_dir = os.path.join(\"data/sign_mnist/test\", cls)\n",
    "        if os.path.exists(cls_dir):\n",
    "            img_files = os.listdir(cls_dir)\n",
    "            img_path = os.path.join(cls_dir, random.choice(img_files))\n",
    "            \n",
    "            # Load and predict\n",
    "            img = Image.open(img_path)\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "                pred_idx = output.argmax(dim=1).item()\n",
    "                confidence = probs[0, pred_idx].item()\n",
    "            \n",
    "            pred_class = checkpoint['classes'][pred_idx]\n",
    "            \n",
    "            # Display\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            color = 'green' if pred_class == cls else 'red'\n",
    "            ax.set_title(f'True: {cls}\\nPred: {pred_class} ({confidence:.2%})', color=color)\n",
    "            ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_cli"
   },
   "source": [
    "## 9. Using the Inference CLI Tool\n",
    "\n",
    "You can also use the command-line inference tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_cli_inference"
   },
   "outputs": [],
   "source": [
    "# Test inference using CLI tool\n",
    "# First, get a sample image path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Get a random test image\n",
    "test_class = random.choice(checkpoint['classes'])\n",
    "test_class_dir = f\"data/sign_mnist/test/{test_class}\"\n",
    "if os.path.exists(test_class_dir):\n",
    "    test_image = os.path.join(test_class_dir, random.choice(os.listdir(test_class_dir)))\n",
    "    print(f\"Testing on image: {test_image}\")\n",
    "    print(f\"True class: {test_class}\\n\")\n",
    "    \n",
    "    !slt-infer --model {OUTPUT_DIR}/model_best.pt --image {test_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## 10. Export and Download Model\n",
    "\n",
    "Download the trained model to use it locally or in other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Download the trained model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading trained model...\")\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/model_best.pt\"):\n",
    "    files.download(f\"{OUTPUT_DIR}/model_best.pt\")\n",
    "    print(\"âœ“ Model downloaded!\")\n",
    "else:\n",
    "    print(\"Model file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_onnx"
   },
   "source": [
    "### Optional: Export to ONNX Format\n",
    "\n",
    "For deployment in production environments, you can export the model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_onnx_model"
   },
   "outputs": [],
   "source": [
    "# Export to ONNX (optional)\n",
    "import torch.onnx\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 1 if checkpoint['grayscale'] else 3, \n",
    "                          checkpoint['image_size'][0], checkpoint['image_size'][1]).to(device)\n",
    "\n",
    "# Export\n",
    "onnx_path = f\"{OUTPUT_DIR}/model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model exported to ONNX: {onnx_path}\")\n",
    "print(f\"File size: {os.path.getsize(onnx_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Download ONNX model\n",
    "files.download(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 11. Next Steps and Tips\n",
    "\n",
    "### Improving Model Performance\n",
    "\n",
    "1. **Try different backbones**: EfficientNet and MobileNet variants may give better results\n",
    "2. **Increase epochs**: Train for 20-30 epochs for better convergence\n",
    "3. **Learning rate tuning**: Try different learning rates (1e-4, 3e-4, 1e-3)\n",
    "4. **Data augmentation**: The training transforms include rotation, scaling, and other augmentations\n",
    "5. **Model ensembling**: Train multiple models and average their predictions\n",
    "\n",
    "### Using the Model Locally\n",
    "\n",
    "1. Download the `model_best.pt` file\n",
    "2. Clone the repository on your local machine\n",
    "3. Install dependencies: `pip install -e .`\n",
    "4. Run inference:\n",
    "   ```bash\n",
    "   slt-infer --model model_best.pt --image your_image.png\n",
    "   # or for webcam\n",
    "   slt-infer --model model_best.pt --webcam\n",
    "   ```\n",
    "\n",
    "### Custom Dataset\n",
    "\n",
    "To train on your own sign language dataset:\n",
    "1. Organize images in ImageFolder format: `data/train/<CLASS>/*.png`\n",
    "2. Adjust `--grayscale` flag based on your images (omit for RGB)\n",
    "3. Modify `--image_size` if needed\n",
    "4. Run training with your custom data directory\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Repository**: https://github.com/NKEthio/Sign_Language_Translator\n",
    "- **Dataset**: https://www.kaggle.com/datamunge/sign-language-mnist\n",
    "- **PyTorch Documentation**: https://pytorch.org/docs/\n",
    "- **Torchvision Models**: https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sign_Language_Translator_Colab.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
